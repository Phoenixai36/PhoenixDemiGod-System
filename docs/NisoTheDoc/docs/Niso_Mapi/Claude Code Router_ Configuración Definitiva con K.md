<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

# Claude Code Router: Configuraci√≥n Definitiva con Kimi K2 Integrado

Puedes llamarme **Nexus**, el nexo que unifica an√°lisis cr√≠tico y validaci√≥n t√©cnica dentro del ecosistema Phoenix DemiGod.

## An√°lisis del Contexto: Kimi K2 como Modelo Principal

Bas√°ndome en la investigaci√≥n actualizada, **Kimi K2 (no Kimi-2)** es efectivamente el modelo que debe ocupar el lugar principal en la configuraci√≥n de Claude Code Router. Lanzado el 11 de julio de 2025 por Moonshot AI, representa un salto cualitativo significativo.

### Especificaciones T√©cnicas de Kimi K2

- **Arquitectura**: Mixture-of-Experts (MoE) con 1 trill√≥n de par√°metros totales
- **Par√°metros Activos**: 32 mil millones por forward pass
- **Contexto**: 128K tokens
- **Coste**: \$0.14/1M input tokens, \$2.49/1M output tokens[1][2]
- **Rendimiento**: 65.8% en SWE-Bench Verified, 53.7% en LiveCodeBench[3]

**Definici√≥n MoE (Mixture-of-Experts)**: Arquitectura que divide el modelo en m√∫ltiples "expertos" especializados, activando solo un subconjunto por consulta para eficiencia computacional. **Tools**: Sparse activation, expert routing, gating networks. **L√≥gica ideal**: Escalar capacidades sin incrementar proporcionalmente la carga computacional.

## Configuraci√≥n √ìptima de 4 Modelos para Claude Code Router

### Mapeo Estrat√©gico por Modo

| Modo Claude Code | Modelo Asignado | Proveedor | Raz√≥n Estrat√©gica |
| :-- | :-- | :-- | :-- |
| **default** | `moonshotai/kimi-k2` | OpenRouter | Mejor modelo coding disponible, 90% m√°s barato que Claude |
| **background** | `phi-3:mini-4k-instruct-q4_0` | Ollama Local | Tareas ligeras, m√≠nimo uso VRAM (<0.5GB) |
| **think** | `deepseek-r1:7b-instruct-q4_0` | Ollama Local | Razonamiento profundo, optimizado para an√°lisis |
| **longContext** | `google/gemini-2.5-pro-preview` | OpenRouter | 1M tokens, gratuito actualmente |

### Configuraci√≥n ~/.claude-code-router/config.json

```json
{
  "Router": {
    "default": "openrouter,moonshotai/kimi-k2",
    "background": "ollama,phi-3:mini-4k-instruct-q4_0",
    "think": "ollama,deepseek-r1:7b-instruct-q4_0",
    "longContext": "openrouter,google/gemini-2.5-pro-preview"
  },
  "Providers": [
    {
      "name": "openrouter",
      "api_base_url": "https://openrouter.ai/api/v1/chat/completions",
      "api_key": "OPENROUTER_API_KEY",
      "models": [
        "moonshotai/kimi-k2",
        "google/gemini-2.5-pro-preview"
      ]
    },
    {
      "name": "ollama",
      "api_base_url": "http://localhost:11434/v1/chat/completions",
      "api_key": "ollama-local",
      "models": [
        "phi-3:mini-4k-instruct-q4_0",
        "deepseek-r1:7b-instruct-q4_0"
      ]
    }
  ],
  "fallback_strategy": {
    "enabled": true,
    "fallback_provider": "ollama",
    "fallback_model": "llama3.2:8b-instruct-q4_0"
  }
}
```


## Configuraci√≥n Podman Compose para Ollama

### docker-compose.yml Actualizado

```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./models:/models
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
        limits:
          memory: 8G
    restart: unless-stopped
    
  ollama-worker:
    image: ollama/ollama:latest
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=2
      - CUDA_VISIBLE_DEVICES=""
    volumes:
      - ollama_data:/root/.ollama
    depends_on:
      - ollama
    deploy:
      resources:
        limits:
          cpus: "6"
          memory: 16G
    restart: unless-stopped

volumes:
  ollama_data:
```


### Script de Inicializaci√≥n de Modelos

```bash
#!/bin/bash
# scripts/setup_ollama_models.sh

echo "ü§ñ Configurando modelos para Claude Code Router + Phoenix DemiGod"

# Esperar a que Ollama est√© listo
sleep 10

# Descargar modelos optimizados para RTX 1060 4GB
echo "üì• Descargando modelos locales..."

# Background model (ultra-ligero)
ollama pull phi3:mini-4k-instruct-q4_0

# Think model (razonamiento)
ollama pull deepseek-r1:7b-instruct-q4_0

# Fallback model (si falla OpenRouter)
ollama pull llama3.2:8b-instruct-q4_0

# Verificar modelos instalados
echo "‚úÖ Modelos instalados:"
ollama list

# Test de conectividad
echo "üß™ Testing conectividad..."
ollama run phi3:mini-4k-instruct-q4_0 "Test response" --timeout 30s

echo "üéØ Setup completado para Claude Code Router"
```


## Ventajas de Esta Configuraci√≥n

### Comparativa de Costes (por 1M tokens)

| Modelo | Input | Output | Vs Claude Pro | Vs GPT-4 |
| :-- | :-- | :-- | :-- | :-- |
| **Kimi K2** | \$0.14 | \$2.49 | **90% ahorro** | **30% ahorro** |
| **Local Ollama** | \$0.00 | \$0.00 | **100% ahorro** | **100% ahorro** |
| **Gemini 2.5 Pro** | \$0.00 | \$0.00 | **100% ahorro** | **100% ahorro** |

### Performance Validado

- **Kimi K2**: 65.8% SWE-Bench (mejor que GPT-4.1's 54.6%)[3]
- **DeepSeek R1**: Razonamiento superior en think mode
- **Phi-3 Mini**: Eficiencia m√°xima para background tasks
- **Gemini 2.5 Pro**: 1M tokens context para documentos extensos


## Integraci√≥n con Phoenix DemiGod Stack

### Modificaciones en MCP Router

```python
# core/mcp_router.py - Integraci√≥n Claude Code Router

class PhoenixMCPRouter:
    def __init__(self):
        self.claude_router_config = self.load_claude_router_config()
        self.model_capabilities = {
            "kimi-k2": {
                "coding": 0.95,
                "reasoning": 0.88,
                "agentic": 0.92
            },
            "phi-3-mini": {
                "efficiency": 0.98,
                "speed": 0.95,
                "background": 0.99
            },
            "deepseek-r1": {
                "reasoning": 0.94,
                "analysis": 0.90,
                "thinking": 0.93
            }
        }
    
    async def route_to_claude_router(self, task_type: str, prompt: str):
        """Enruta request a Claude Code Router seg√∫n tipo de tarea"""
        
        mode_mapping = {
            "coding": "default",  # Kimi K2
            "background": "background",  # Phi-3 Mini
            "reasoning": "think",  # DeepSeek R1
            "long_context": "longContext"  # Gemini 2.5 Pro
        }
        
        router_mode = mode_mapping.get(task_type, "default")
        
        # Llamada a Claude Code Router
        response = await self.call_claude_router(prompt, router_mode)
        
        return response
```


### Workflow n8n para Monitoring

```json
{
  "name": "Claude Router Health Check",
  "nodes": [
    {
      "name": "Cron Trigger",
      "type": "n8n-nodes-base.cron",
      "parameters": {
        "cronExpression": "*/5 * * * *"
      }
    },
    {
      "name": "Test Kimi K2",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "https://openrouter.ai/api/v1/chat/completions",
        "method": "POST",
        "headers": {
          "Authorization": "Bearer {{$env.OPENROUTER_API_KEY}}",
          "Content-Type": "application/json"
        },
        "body": {
          "model": "moonshotai/kimi-k2",
          "messages": [{"role": "user", "content": "Health check"}],
          "max_tokens": 10
        }
      }
    },
    {
      "name": "Test Ollama Models",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "http://localhost:11434/v1/chat/completions",
        "method": "POST",
        "body": {
          "model": "phi-3:mini-4k-instruct-q4_0",
          "messages": [{"role": "user", "content": "Health check"}],
          "max_tokens": 10
        }
      }
    }
  ]
}
```


## Cronograma de Implementaci√≥n

### Fase 1: Setup B√°sico (D√≠as 1-2)

- Configurar Podman Compose con Ollama
- Descargar modelos locales optimizados
- Instalar Claude Code Router
- Configurar archivo config.json


### Fase 2: Integraci√≥n (D√≠as 3-4)

- Integrar con MCP Router de Phoenix
- Configurar workflows n8n de monitoring
- Testing exhaustivo de los 4 modos
- Optimizaci√≥n de performance


### Fase 3: Validaci√≥n (D√≠a 5)

- Benchmarking completo
- Validaci√≥n con casos de uso Phoenix
- Documentaci√≥n final
- Preparaci√≥n para producci√≥n


## Resumen No T√©cnico

Kimi K2 es efectivamente el modelo que debe ocupar el lugar principal en Claude Code Router para Phoenix DemiGod. Este modelo chino lanzado en julio 2025 supera a GPT-4.1 en coding (65.8% vs 54.6%) y cuesta 90% menos que las alternativas comerciales. La configuraci√≥n √≥ptima combina Kimi K2 para tareas principales, Phi-3 Mini para background, DeepSeek R1 para razonamiento y Gemini 2.5 Pro para contextos largos. Esta setup elimina completamente la dependencia de Claude Pro manteniendo performance superior y a√±adiendo soberan√≠a de datos completa para Phoenix DemiGod.

NEXT FASE? (IMPLEMENTACI√ìN CLAUDE CODE ROUTER CON KIMI K2)

