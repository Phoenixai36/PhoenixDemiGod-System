<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

## 25. `src/core/nlp/quantum_attention.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Quantum Attention Module for Phoenix DemiGod

This module implements quantum-inspired attention mechanisms for natural
language processing tasks, enabling superposition of semantic states.
"""

import logging
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/quantum_attention.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("QuantumAttention")

class QuantumAttention(nn.Module):
    """
    Quantum-inspired attention mechanism that processes information using
    principles inspired by quantum mechanics.
    
    Attributes:
        embed_dim (int): Embedding dimension
        num_heads (int): Number of attention heads
        dropout (float): Dropout probability
        entanglement (bool): Whether to use entanglement between heads
        superposition (bool): Whether to use superposition of attention states
    """
    
    def __init__(
        self,
        embed_dim: int = 768,
        num_heads: int = 8,
        dropout: float = 0.1,
        entanglement: bool = True,
        superposition: bool = True
    ):
        """
        Initialize the quantum attention module.
        
        Args:
            embed_dim: Embedding dimension
            num_heads: Number of attention heads
            dropout: Dropout probability
            entanglement: Whether to use entanglement between heads
            superposition: Whether to use superposition of attention states
        """
        super().__init__()
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.entanglement = entanglement
        self.superposition = superposition
        
        # Ensure head_dim is divisible
        if self.head_dim * num_heads != embed_dim:
            raise ValueError(f"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})")
        
        # Linear projections for queries, keys, values
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        # Phase encoding for quantum simulation
        self.phase_encoding = nn.Parameter(torch.randn(num_heads, self.head_dim) * 0.02)
        
        # Hadamard transformation (quantum-inspired)
        h = np.array([[1, 1], [1, -1]]) / np.sqrt(2)
        
        # Build Hadamard matrix of appropriate size
        n = int(np.log2(self.head_dim))
        if 2**n != self.head_dim:
            # If head_dim is not a power of 2, use the closest power of 2
            n = int(np.ceil(np.log2(self.head_dim)))
            h_matrix = np.zeros((2**n, 2**n))
            h_matrix[:self.head_dim, :self.head_dim] = np.kron(*([h] * n))[:self.head_dim, :self.head_dim]
        else:
            h_matrix = np.kron(*([h] * n))
        
        # Register Hadamard matrix as non-trainable parameter
        self.register_buffer('hadamard', torch.tensor(h_matrix, dtype=torch.float32))
        
        self.dropout = nn.Dropout(dropout)
        
        logger.info(f"Initialized QuantumAttention with {num_heads} heads, dimension {embed_dim}")
    
    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass for quantum attention.
        
        Args:
            query: Query embeddings (batch_size, seq_len_q, embed_dim)
            key: Key embeddings (batch_size, seq_len_k, embed_dim)
            value: Value embeddings (batch_size, seq_len_v, embed_dim)
            attn_mask: Attention mask (seq_len_q, seq_len_k)
            key_padding_mask: Key padding mask (batch_size, seq_len_k)
            
        Returns:
            Tuple of (output, attention weights)
        """
        batch_size, seq_len_q, _ = query.shape
        _, seq_len_k, _ = key.shape
        
        # Linear projections
        q = self.q_proj(query)
        k = self.k_proj(key)
        v = self.v_proj(value)
        
        # Reshape for multi-head attention
        q = q.view(batch_size, seq_len_q, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len_k, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Apply quantum transformations
        if self.superposition:
            q = self._apply_superposition(q)
            k = self._apply_superposition(k)
        
        # Apply phase encoding (quantum-inspired)
        q = q * torch.exp(1j * self.phase_encoding.unsqueeze(1).unsqueeze(0))
        k = k * torch.exp(-1j * self.phase_encoding.unsqueeze(1).unsqueeze(0))
        
        # Take real part after phase encoding
        q = torch.real(q)
        k = torch.real(k)
        
        # Calculate attention scores
        # Scale dot product attention
        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)
        
        # Apply attention mask if provided
        if attn_mask is not None:
            attn_weights = attn_weights + attn_mask.unsqueeze(0).unsqueeze(0)
        
        # Apply key padding mask if provided
        if key_padding_mask is not None:
            attn_weights = attn_weights.masked_fill(
                key_padding_mask.unsqueeze(1).unsqueeze(2),
                float('-inf')
            )
        
        # Apply softmax and dropout
        attn_probs = F.softmax(attn_weights, dim=-1)
        attn_probs = self.dropout(attn_probs)
        
        # Apply entanglement between heads if enabled
        if self.entanglement:
            attn_probs = self._apply_entanglement(attn_probs)
        
        # Apply attention to values
        context = torch.matmul(attn_probs, v)
        
        # Reshape and project output
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.embed_dim)
        output = self.out_proj(context)
        
        return output, attn_probs
    
    def _apply_superposition(self, x: torch.Tensor) -> torch.Tensor:
        """
        Apply quantum-inspired superposition using Hadamard transformation.
        
        Args:
            x: Input tensor (batch_size, num_heads, seq_len, head_dim)
            
        Returns:
            Tensor with applied superposition
        """
        batch_size, num_heads, seq_len, _ = x.shape
        
        # Reshape for hadamard multiplication
        x_reshaped = x.view(-1, self.head_dim)
        
        # Apply Hadamard transformation
        x_superposition = torch.matmul(x_reshaped, self.hadamard.to(x.device))
        
        # Reshape back
        return x_superposition.view(batch_size, num_heads, seq_len, self.head_dim)
    
    def _apply_entanglement(self, attn_probs: torch.Tensor) -> torch.Tensor:
        """
        Apply quantum-inspired entanglement between attention heads.
        
        Args:
            attn_probs: Attention probabilities (batch_size, num_heads, seq_len_q, seq_len_k)
            
        Returns:
            Entangled attention probabilities
        """
        batch_size, num_heads, seq_len_q, seq_len_k = attn_probs.shape
        
        # Create entanglement mask (randomly entangle pairs of heads)
        if not hasattr(self, 'entanglement_mask') or self.training:
            # During training, create a new entanglement mask each time
            entanglement_mask = torch.eye(num_heads, device=attn_probs.device)
            # Randomly pair heads for entanglement
            perm = torch.randperm(num_heads)
            for i in range(0, num_heads - 1, 2):
                h1, h2 = perm[i], perm[i + 1]
                # Create entanglement between these heads
                entanglement_mask[h1, h2] = 0.2
                entanglement_mask[h2, h1] = 0.2
            
            if self.training:
                self.entanglement_mask = entanglement_mask
            else:
                # During evaluation, use a fixed entanglement mask
                self.register_buffer('entanglement_mask', entanglement_mask)
        
        # Apply entanglement by mixing attention probabilities between heads
        entangled_probs = torch.einsum('bhqk,hg->bgqk', attn_probs, self.entanglement_mask)
        
        return entangled_probs


class QuantumEnhancedTransformerEncoder(nn.Module):
    """
    Transformer encoder with quantum-enhanced attention mechanism.
    
    Attributes:
        embed_dim (int): Embedding dimension
        num_heads (int): Number of attention heads
        feedforward_dim (int): Dimension of feedforward network
        dropout (float): Dropout probability
        num_layers (int): Number of encoder layers
        quantum_layers (List[int]): Indices of layers using quantum attention
    """
    
    def __init__(
        self,
        embed_dim: int = 768,
        num_heads: int = 8,
        feedforward_dim: int = 2048,
        dropout: float = 0.1,
        num_layers: int = 6,
        quantum_layers: List[int] = None
    ):
        """
        Initialize the quantum-enhanced transformer encoder.
        
        Args:
            embed_dim: Embedding dimension
            num_heads: Number of attention heads
            feedforward_dim: Dimension of feedforward network
            dropout: Dropout probability
            num_layers: Number of encoder layers
            quantum_layers: Indices of layers using quantum attention (default: all layers)
        """
        super().__init__()
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        
        # Default to all layers using quantum attention if not specified
        if quantum_layers is None:
            quantum_layers = list(range(num_layers))
        self.quantum_layers = quantum_layers
        
        # Build encoder layers
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            if i in quantum_layers:
                attn = QuantumAttention(embed_dim, num_heads, dropout)
            else:
                attn = nn.MultiheadAttention(embed_dim, num_heads, dropout)
            
            # Create encoder layer
            layer = EncoderLayer(
                attn=attn,
                embed_dim=embed_dim,
                feedforward_dim=feedforward_dim,
                dropout=dropout,
                is_quantum=i in quantum_layers
            )
            self.layers.append(layer)
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(embed_dim)
        
        logger.info(f"Initialized QuantumEnhancedTransformerEncoder with {num_layers} layers")
        logger.info(f"Quantum attention in layers: {quantum_layers}")
    
    def forward(
        self,
        src: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass for quantum-enhanced transformer encoder.
        
        Args:
            src: Source embeddings (batch_size, seq_len, embed_dim)
            mask: Attention mask (seq_len, seq_len)
            src_key_padding_mask: Source key padding mask (batch_size, seq_len)
            
        Returns:
            Encoded representation
        """
        output = src
        
        # Process through each encoder layer
        for i, layer in enumerate(self.layers):
            output = layer(
                output,
                mask=mask,
                src_key_padding_mask=src_key_padding_mask
            )
        
        # Apply final layer normalization
        output = self.layer_norm(output)
        
        return output


class EncoderLayer(nn.Module):
    """
    Transformer encoder layer with support for both regular and quantum attention.
    
    Attributes:
        attn (nn.Module): Attention module (regular or quantum)
        embed_dim (int): Embedding dimension
        feedforward_dim (int): Dimension of feedforward network
        dropout (float): Dropout probability
        is_quantum (bool): Whether this layer uses quantum attention
    """
    
    def __init__(
        self,
        attn: nn.Module,
        embed_dim: int = 768,
        feedforward_dim: int = 2048,
        dropout: float = 0.1,
        is_quantum: bool = True
    ):
        """
        Initialize the encoder layer.
        
        Args:
            attn: Attention module (regular or quantum)
            embed_dim: Embedding dimension
            feedforward_dim: Dimension of feedforward network
            dropout: Dropout probability
            is_quantum: Whether this layer uses quantum attention
        """
        super().__init__()
        
        self.attn = attn
        self.is_quantum = is_quantum
        
        # Feed-forward network
        self.linear1 = nn.Linear(embed_dim, feedforward_dim)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(feedforward_dim, embed_dim)
        
        # Layer normalization
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
    
    def forward(
        self,
        src: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass for encoder layer.
        
        Args:
            src: Source embeddings (batch_size, seq_len, embed_dim)
            mask: Attention mask (seq_len, seq_len)
            src_key_padding_mask: Source key padding mask (batch_size, seq_len)
            
        Returns:
            Encoded representation
        """
        # Apply attention
        if self.is_quantum:
            # Quantum attention expects (batch_size, seq_len, embed_dim)
            attn_output, _ = self.attn(
                query=src,
                key=src,
                value=src,
                attn_mask=mask,
                key_padding_mask=src_key_padding_mask
            )
        else:
            # Regular MultiheadAttention expects (seq_len, batch_size, embed_dim)
            src_transposed = src.transpose(0, 1)
            attn_output, _ = self.attn(
                query=src_transposed,
                key=src_transposed,
                value=src_transposed,
                attn_mask=mask,
                key_padding_mask=src_key_padding_mask
            )
            # Convert back to (batch_size, seq_len, embed_dim)
            attn_output = attn_output.transpose(0, 1)
        
        # Add & Norm (first residual connection)
        src = src + self.dropout1(attn_output)
        src = self.norm1(src)
        
        # Feed Forward
        ff_output = self.linear2(self.dropout(F.relu(self.linear1(src))))
        
        # Add & Norm (second residual connection)
        src = src + self.dropout2(ff_output)
        src = self.norm2(src)
        
        return src


if __name__ == "__main__":
    # Example usage
    batch_size = 4
    seq_len = 16
    embed_dim = 256
    num_heads = 8
    
    # Create random input
    x = torch.rand(batch_size, seq_len, embed_dim)
    
    # Initialize quantum attention module
    quantum_attn = QuantumAttention(embed_dim, num_heads)
    
    # Forward pass
    output, attn_weights = quantum_attn(x, x, x)
    
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Attention weights shape: {attn_weights.shape}")
    
    # Create a quantum-enhanced transformer encoder
    encoder = QuantumEnhancedTransformerEncoder(
        embed_dim=embed_dim,
        num_heads=num_heads,
        num_layers=3,
        quantum_layers=[0, 2]
    )
    
    # Forward pass through encoder
    encoded = encoder(x)
    
    print(f"Encoded output shape: {encoded.shape}")
```


## 26. `src/core/analytics/topological_data_analysis.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Topological Data Analysis Module for Phoenix DemiGod

This module implements tools for analyzing data using topological methods,
enabling the detection of complex patterns and structures in high-dimensional data.
"""

import logging
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Tuple, Union, Callable
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import linkage, fcluster

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/topological_analysis.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("TopologicalAnalysis")

class PersistentHomology:
    """
    Implementation of persistent homology for topological data analysis.
    
    Attributes:
        max_dimension (int): Maximum homology dimension to compute
        metric (str): Distance metric to use
        verbose (bool): Whether to print verbose output
    """
    
    def __init__(
        self,
        max_dimension: int = 1,
        metric: str = 'euclidean',
        verbose: bool = False
    ):
        """
        Initialize the persistent homology analyzer.
        
        Args:
            max_dimension: Maximum homology dimension to compute
            metric: Distance metric to use
            verbose: Whether to print verbose output
        """
        self.max_dimension = max_dimension
        self.metric = metric
        self.verbose = verbose
        self.diagrams = None
        
        logger.info(f"Initialized PersistentHomology with max_dimension={max_dimension}, metric={metric}")
    
    def fit(self, X: np.ndarray) -> 'PersistentHomology':
        """
        Compute persistent homology for the given data.
        
        Args:
            X: Input data array (n_samples, n_features)
            
        Returns:
            Self
        """
        if self.verbose:
            logger.info(f"Computing persistent homology for {X.shape[^0]} points in {X.shape[^1]} dimensions")
        
        # Compute distance matrix
        dist_matrix = squareform(pdist(X, metric=self.metric))
        
        # Compute persistence diagrams for each dimension
        self.diagrams = {}
        
        for dim in range(self.max_dimension + 1):
            if self.verbose:
                logger.info(f"Computing {dim}-dimensional persistence")
            
            # Compute filtration
            filtration = self._compute_filtration(dist_matrix, dim)
            
            # Compute persistence diagram
            diagram = self._compute_persistence(filtration, dim)
            
            self.diagrams[dim] = diagram
            
            if self.verbose:
                logger.info(f"Found {len(diagram)} {dim}-dimensional features")
        
        return self
    
    def _compute_filtration(self, dist_matrix: np.ndarray, dimension: int) -> List[Tuple]:
        """
        Compute the filtration for the given dimension.
        
        Args:
            dist_matrix: Distance matrix
            dimension: Homology dimension
            
        Returns:
            List of simplices in the filtration
        """
        n_samples = dist_matrix.shape[^0]
        
        # Start with 0-simplices (vertices)
        filtration = [(i,) for i in range(n_samples)]
        
        # For 0-dimensional homology, we only need vertices
        if dimension == 0:
            return filtration
        
        # Compute higher dimensional simplices
        # For 1-dimensional homology, we need edges (pairs of vertices)
        if dimension >= 1:
            # Build edges based on the distance matrix
            edges = []
            for i in range(n_samples):
                for j in range(i+1, n_samples):
                    edges.append(((i, j), dist_matrix[i, j]))
            
            # Sort edges by distance
            edges.sort(key=lambda x: x[^1])
            
            # Add edges to filtration
            for (i, j), dist in edges:
                filtration.append((i, j))
        
        # For 2-dimensional homology, we need faces (triplets of vertices)
        if dimension >= 2:
            # Build faces based on edges
            faces = []
            
            # For each possible triplet (i,j,k), check if all edges exist
            edge_set = set(e for e, _ in edges)
            for i in range(n_samples):
                for j in range(i+1, n_samples):
                    for k in range(j+1, n_samples):
                        if (i, j) in edge_set and (j, k) in edge_set and (i, k) in edge_set:
                            # Compute the maximum distance among the three edges
                            max_dist = max(dist_matrix[i, j], dist_matrix[j, k], dist_matrix[i, k])
                            faces.append(((i, j, k), max_dist))
            
            # Sort faces by maximum distance
            faces.sort(key=lambda x: x[^1])
            
            # Add faces to filtration
            for (i, j, k), _ in faces:
                filtration.append((i, j, k))
        
        # Higher dimensions would follow a similar pattern
        
        return filtration
    
    def _compute_persistence(self, filtration: List[Tuple], dimension: int) -> List[Tuple[float, float]]:
        """
        Compute the persistence diagram for the given filtration and dimension.
        
        Args:
            filtration: List of simplices in the filtration
            dimension: Homology dimension
            
        Returns:
            Persistence diagram (list of birth-death pairs)
        """
        # This is a simplified implementation of persistence computation
        # In a real implementation, we would use a dedicated package like Dionysus or GUDHI
        
        if dimension == 0:
            # For 0-dimensional homology, we can use a simple union-find data structure
            # to track connected components
            
            # Initialize each vertex as its own component
            n_vertices = len([s for s in filtration if len(s) == 1])
            parent = list(range(n_vertices))
            birth = [0.0] * n_vertices  # All vertices are born at time 0
            death = [float('inf')] * n_vertices  # Initially, all components persist forever
            
            def find(x):
                if parent[x] != x:
                    parent[x] = find(parent[x])
                return parent[x]
            
            def union(x, y, time):
                x_root = find(x)
                y_root = find(y)
                
                if x_root == y_root:
                    return
                
                # The younger component dies and merges into the older one
                if birth[x_root] > birth[y_root]:
                    x_root, y_root = y_root, x_root
                
                # Record death time of the younger component
                death[y_root] = time
                
                # Merge components
                parent[y_root] = x_root
            
            # Process edges to merge connected components
            time = 0.0
            for simplex in filtration:
                if len(simplex) == 2:  # Edge
                    i, j = simplex
                    time += 0.01  # Increment time for each edge
                    union(i, j, time)
            
            # Collect birth-death pairs
            diagram = []
            for i in range(n_vertices):
                if find(i) == i:  # Root of a component
                    diagram.append((birth[i], float('inf')))  # The oldest component persists forever
                else:
                    if death[i] < float('inf'):
                        diagram.append((birth[i], death[i]))
            
            return diagram
        
        else:
            # For higher dimensions, we would need a proper implementation of
            # persistent homology computation, which is beyond the scope of this example
            
            # Return a placeholder
            return []
    
    def plot_diagram(self, dimension: int = 0, ax=None, max_death: float = None) -> None:
        """
        Plot the persistence diagram for the given dimension.
        
        Args:
            dimension: Homology dimension
            ax: Matplotlib axis (if None, a new one is created)
            max_death: Maximum death time to plot (if None, automatically determined)
        """
        if self.diagrams is None or dimension not in self.diagrams:
            raise ValueError("Persistence diagram not computed or dimension not available")
        
        diagram = self.diagrams[dimension]
        
        if not diagram:
            logger.warning(f"No {dimension}-dimensional features found")
            return
        
        if ax is None:
            _, ax = plt.subplots(figsize=(6, 6))
        
        # Extract birth and death times
        births = [b for b, d in diagram]
        deaths = [d if d < float('inf') else max_death for b, d in diagram]
        
        if max_death is None:
            max_death = max(deaths) * 1.1
        
        # Plot points
        ax.scatter(births, deaths, s=50, alpha=0.7)
        
        # Plot diagonal
        diag_min = min(births) if births else 0
        diag_max = max_death
        ax.plot([diag_min, diag_max], [diag_min, diag_max], 'k--', alpha=0.5)
        
        # Labels
        ax.set_xlabel('Birth')
        ax.set_ylabel('Death')
        ax.set_title(f'{dimension}-dimensional Persistence Diagram')
        
        # Set equal aspect ratio
        ax.set_aspect('equal', 'box')
        
        # Set limits
        ax.set_xlim(diag_min, diag_max)
        ax.set_ylim(diag_min, diag_max)
        
        return ax
    
    def plot_barcode(self, dimension: int = 0, ax=None, max_death: float = None) -> None:
        """
        Plot the persistence barcode for the given dimension.
        
        Args:
            dimension: Homology dimension
            ax: Matplotlib axis (if None, a new one is created)
            max_death: Maximum death time to plot (if None, automatically determined)
        """
        if self.diagrams is None or dimension not in self.diagrams:
            raise ValueError("Persistence diagram not computed or dimension not available")
        
        diagram = self.diagrams[dimension]
        
        if not diagram:
            logger.warning(f"No {dimension}-dimensional features found")
            return
        
        if ax is None:
            _, ax = plt.subplots(figsize=(8, 6))
        
        # Sort features by persistence (death - birth)
        sorted_features = sorted(
            diagram,
            key=lambda x: x[^1] - x[^0] if x[^1] < float('inf') else float('inf'),
            reverse=True
        )
        
        # Set maximum death time for visualization
        if max_death is None:
            finite_deaths = [d for _, d in diagram if d < float('inf')]
            max_death = max(finite_deaths) * 1.1 if finite_deaths else 1.0
        
        # Plot bars
        for i, (birth, death) in enumerate(sorted_features):
            if death == float('inf'):
                ax.plot([birth, max_death], [i, i], 'k-', linewidth=2)
            else:
                ax.plot([birth, death], [i, i], 'k-', linewidth=2)
        
        # Labels
        ax.set_xlabel('Filtration Value')
        ax.set_ylabel('Features (sorted by persistence)')
        ax.set_title(f'{dimension}-dimensional Persistence Barcode')
        
        return ax
    
    def get_persistent_features(
        self,
        dimension: int = 0,
        persistence_threshold: float = 0.1
    ) -> List[Tuple[float, float]]:
        """
        Get the persistent features for the given dimension.
        
        Args:
            dimension: Homology dimension
            persistence_threshold: Minimum persistence value to consider a feature
            
        Returns:
            List of persistent features (birth, death)
        """
        if self.diagrams is None or dimension not in self.diagrams:
            raise ValueError("Persistence diagram not computed or dimension not available")
        
        diagram = self.diagrams[dimension]
        
        # Filter features by persistence (death - birth)
        persistent_features = []
        for birth, death in diagram:
            if death == float('inf') or death - birth >= persistence_threshold:
                persistent_features.append((birth, death))
        
        return persistent_features
    
    def get_persistence_landscape(
        self,
        dimension: int = 0,
        num_landscapes: int = 1,
        resolution: int = 100,
        max_value: float = None
    ) -> np.ndarray:
        """
        Compute the persistence landscape for the given dimension.
        
        Args:
            dimension: Homology dimension
            num_landscapes: Number of landscape layers to compute
            resolution: Resolution of the landscape
            max_value: Maximum filtration value (if None, automatically determined)
            
        Returns:
            Landscape array of shape (num_landscapes, resolution)
        """
        if self.diagrams is None or dimension not in self.diagrams:
            raise ValueError("Persistence diagram not computed or dimension not available")
        
        diagram = self.diagrams[dimension]
        
        if not diagram:
            return np.zeros((num_landscapes, resolution))
        
        # Determine the range of filtration values
        births = [b for b, _ in diagram]
        finite_deaths = [d for _, d in diagram if d < float('inf')]
        
        if not finite_deaths:
            max_death = max(births) * 2.0
        else:
            max_death = max(finite_deaths)
        
        if max_value is None:
            max_value = max_death * 1.1
        
        # Create a grid of x values
        grid = np.linspace(0, max_value, resolution)
        
        # Initialize the landscape
        landscape = np.zeros((num_landscapes, resolution))
        
        # For each point in the persistence diagram, add a tent function to the landscape
        for birth, death in diagram:
            if death == float('inf'):
                death = max_value
            
            # Compute tent function
            peak = (birth + death) / 2
            
            for i in range(resolution):
                x = grid[i]
                if birth <= x <= peak:
                    value = x - birth
                elif peak <= x <= death:
                    value = death - x
                else:
                    value = 0
                
                # Add to the landscape
                for j in range(num_landscapes):
                    if value > landscape[j, i]:
                        # Shift down existing values
                        if j + 1 < num_landscapes:
                            landscape[j+1:, i] = np.roll(landscape[j:-1, i], 1)
                        
                        landscape[j, i] = value
                        break
        
        return landscape


class Mapper:
    """
    Implementation of the Mapper algorithm for topological data analysis.
    
    Attributes:
        filter_function (Callable): Function to map data points to filter values
        num_intervals (int): Number of intervals for the filter function
        overlap_fraction (float): Fraction of overlap between intervals
        clusterer (object): Clustering algorithm to use for each fiber
        cover (List): Cover of the filter range
    """
    
    def __init__(
        self,
        filter_function: Callable = None,
        num_intervals: int = 10,
        overlap_fraction: float = 0.5,
        clusterer=None
    ):
        """
        Initialize the Mapper algorithm.
        
        Args:
            filter_function: Function to map data points to filter values
            num_intervals: Number of intervals for the filter function
            overlap_fraction: Fraction of overlap between intervals
            clusterer: Clustering algorithm to use for each fiber
        """
        self.filter_function = filter_function
        self.num_intervals = num_intervals
        self.overlap_fraction = overlap_fraction
        
        # Default to hierarchical clustering if not provided
        if clusterer is None:
            from sklearn.cluster import AgglomerativeClustering
            self.clusterer = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5)
        else:
            self.clusterer = clusterer
        
        self.cover = None
        self.graph = None
        
        logger.info(f"Initialized Mapper with {num_intervals} intervals and {overlap_fraction} overlap")
    
    def fit_transform(self, X: np.ndarray) -> Dict:
        """
        Apply the Mapper algorithm to the data.
        
        Args:
            X: Input data array (n_samples, n_features)
            
        Returns:
            Graph representation of the data
        """
        # Apply filter function to get filter values
        if self.filter_function is None:
            # Default to the first principal component
            from sklearn.decomposition import PCA
            pca = PCA(n_components=1)
            filter_values = pca.fit_transform(X).flatten()
            logger.info("Using PCA as default filter function")
        else:
            filter_values = self.filter_function(X)
        
        # Create cover of the filter range
        filter_min = filter_values.min()
        filter_max = filter_values.max()
        
        interval_length = (filter_max - filter_min) / self.num_intervals
        overlap = interval_length * self.overlap_fraction
        
        self.cover = []
        for i in range(self.num_intervals):
            interval_min = filter_min + i * interval_length - overlap / 2
            interval_max = filter_min + (i + 1) * interval_length + overlap / 2
            
            # Ensure the intervals don't exceed the filter range
            interval_min = max(interval_min, filter_min)
            interval_max = min(interval_max, filter_max)
            
            self.cover.append((interval_min, interval_max))
        
        # Process each interval
        logger.info("Processing intervals...")
        nodes = {}
        edges = []
        
        for i, (interval_min, interval_max) in enumerate(self.cover):
            # Get points in this interval
            mask = (filter_values >= interval_min) & (filter_values <= interval_max)
            if not np.any(mask):
                continue
            
            points_indices = np.where(mask)[^0]
            points = X[points_indices]
            
            # Cluster points in this interval
            if len(points) <= 1:
                # Only one point, no need to cluster
                clusters = [^0]
            else:
                try:
                    # Try to use the provided clusterer
                    clusters = self.clusterer.fit_predict(points)
                except Exception as e:
                    # If the clusterer doesn't support precomputed distance matrix
                    logger.warning(f"Clustering failed: {e}. Using single cluster.")
                    clusters = np.zeros(len(points), dtype=int)
            
            # Create nodes for each cluster
            for cluster_id in np.unique(clusters):
                node_id = f"{i}_{cluster_id}"
                cluster_indices = points_indices[clusters == cluster_id]
                
                nodes[node_id] = {
                    "interval": i,
                    "cluster": cluster_id,
                    "points": cluster_indices.tolist(),
                    "size": len(cluster_indices)
                }
        
        # Create edges between nodes with common points
        for node1_id, node1 in nodes.items():
            for node2_id, node2 in nodes.items():
                if node1_id >= node2_id:  # Avoid duplicate edges
                    continue
                
                # Check if the nodes share any points
                common_points = set(node1["points"]) & set(node2["points"])
                if common_points:
                    edges.append({
                        "source": node1_id,
                        "target": node2_id,
                        "weight": len(common_points)
                    })
        
        self.graph = {
            "nodes": nodes,
            "edges": edges
        }
        
        logger.info(f"Mapper graph created with {len(nodes)} nodes and {len(edges)} edges")
        
        return self.graph
    
    def plot_graph(self, ax=None, node_color_map: Dict = None, layout: str = 'spring') -> None:
        """
        Plot the Mapper graph.
        
        Args:
            ax: Matplotlib axis (if None, a new one is created)
            node_color_map: Dictionary mapping node IDs to colors
            layout: Graph layout algorithm ('spring', 'spectral', 'circular')
        """
        if self.graph is None:
            raise ValueError("Mapper graph not computed")
        
        try:
            import networkx as nx
        except ImportError:
            logger.error("Networkx is required for plotting Mapper graphs")
            raise
        
        # Create networkx graph
        G = nx.Graph()
        
        # Add nodes
        for node_id, node_data in self.graph["nodes"].items():
            G.add_node(node_id, **node_data)
        
        # Add edges
        for edge in self.graph["edges"]:
            G.add_edge(edge["source"], edge["target"], weight=edge["weight"])
        
        # Create layout
        if layout == 'spring':
            pos = nx.spring_layout(G)
        elif layout == 'spectral':
            pos = nx.spectral_layout(G)
        elif layout == 'circular':
            pos = nx.circular_layout(G)
        else:
            raise ValueError(f"Unknown layout: {layout}")
        
        # Create plot
        if ax is None:
            _, ax = plt.subplots(figsize=(10, 8))
        
        # Node sizes based on number of points
        node_sizes = [node_data["size"] * 10 for node_data in self.graph["nodes"].values()]
        
        # Node colors
        if node_color_map is None:
            # Color nodes by interval
            node_colors = [node_data["interval"] for node_data in self.graph["nodes"].values()]
        else:
            node_colors = [node_color_map[node_id] for node_id in self.graph["nodes"].keys()]
        
        # Edge weights
        edge_weights = [edge["weight"] for edge in self.graph["edges"]]
        
        # Draw the graph
        nx.draw_networkx(
            G,
            pos=pos,
            with_labels=False,
            node_size=node_sizes,
            node_color=node_colors,
            cmap=plt.cm.viridis,
            width=edge_weights,
            alpha=0.8,
            ax=ax
        )
        
        ax.set_title('Mapper Graph')
        ax.axis('off')
        
        return ax


if __name__ == "__main__":
    # Example usage
    from sklearn.datasets import make_circles
    
    # Generate example data
    X, y = make_circles(n_samples=100, noise=0.05, factor=0.3, random_state=42)
    
    # Persistent Homology example
    ph = PersistentHomology(max_dimension=1, verbose=True)
    ph.fit(X)
    
    # Plot persistence diagram
    plt.figure(figsize=(10, 5))
    
    plt.subplot(1, 2, 1)
    ph.plot_diagram(dimension=0)
    
    plt.subplot(1, 2, 2)
    ph.plot_barcode(dimension=0)
    
    plt.tight_layout()
    plt.savefig("persistence_analysis.png")
    
    # Mapper example
    mapper = Mapper(num_intervals=5, overlap_fraction=0.3)
    graph = mapper.fit_transform(X)
    
    # Plot mapper graph
    plt.figure(figsize=(8, 8))
    node_color_map = {node_id: y[node_data["points"][^0]] for node_id, node_data in graph["nodes"].items()}
    mapper.plot_graph(node_color_map=node_color_map)
    
    plt.savefig("mapper_graph.png")
```


## 27. `src/core/training/exponential_learning.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Exponential Learning Module for Phoenix DemiGod

This module implements exponential learning algorithms that accelerate training
by adaptively adjusting the learning rate, sample selection, and model complexity
based on learning curves and performance metrics.
"""

import logging
import math
import numpy as np
import time
from typing import Dict, List, Optional, Tuple, Union, Callable

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/exponential_learning.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("ExponentialLearning")

class ComplexityScheduler:
    """
    Dynamically adjusts model complexity based on training progress.
    
    Attributes:
        initial_complexity (float): Initial complexity factor
        max_complexity (float): Maximum complexity factor
        growth_rate (float): Rate at which complexity increases
        patience (int): Number of evaluations with no improvement before adjusting
        min_delta (float): Minimum change in metric to be considered improvement
        model_adapter (Callable): Function to adapt model complexity
    """
    
    def __init__(
        self,
        initial_complexity: float = 0.3,
        max_complexity: float = 1.0,
        growth_rate: float = 0.1,
        patience: int = 5,
        min_delta: float = 0.001,
        model_adapter: Callable = None
    ):
        """
        Initialize the complexity scheduler.
        
        Args:
            initial_complexity: Initial complexity factor
            max_complexity: Maximum complexity factor
            growth_rate: Rate at which complexity increases
            patience: Number of evaluations with no improvement before adjusting
            min_delta: Minimum change in metric to be considered improvement
            model_adapter: Function to adapt model complexity
        """
        self.complexity = initial_complexity
        self.max_complexity = max_complexity
        self.growth_rate = growth_rate
        self.patience = patience
        self.min_delta = min_delta
        self.model_adapter = model_adapter
        
        self.current_patience = patience
        self.best_metric = float('-inf')
        
        logger.info(f"Initialized ComplexityScheduler with initial_complexity={initial_complexity}, "
                  f"max_complexity={max_complexity}, growth_rate={growth_rate}")
    
    def step(self, metric: float, model: nn.Module) -> nn.Module:
        """
        Update complexity based on performance metric.
        
        Args:
            metric: Performance metric value
            model: Model to adapt
            
        Returns:
            Adapted model
        """
        # Check if there's improvement
        delta = metric - self.best_metric
        
        if delta > self.min_delta:
            # Improvement detected
            self.best_metric = metric
            self.current_patience = self.patience
            logger.info(f"Performance improved to {metric:.6f}")
            
            # If we're already at max complexity, no need to adapt
            if self.complexity >= self.max_complexity:
                return model
        else:
            # No improvement
            self.current_patience -= 1
            logger.info(f"No improvement detected. Patience: {self.current_patience}")
            
            if self.current_patience <= 0 and self.complexity < self.max_complexity:
                # Increase complexity
                old_complexity = self.complexity
                self.complexity = min(self.max_complexity, 
                                     self.complexity + self.growth_rate)
                
                logger.info(f"Increasing complexity from {old_complexity:.2f} to {self.complexity:.2f}")
                
                # Reset patience
                self.current_patience = self.patience
                
                # Adapt model if adapter is provided
                if self.model_adapter is not None:
                    model = self.model_adapter(model, self.complexity)
        
        return model
    
    def get_current_complexity(self) -> float:
        """
        Get the current complexity factor.
        
        Returns:
            Current complexity factor
        """
        return self.complexity


class CurriculumLearning:
    """
    Implements curriculum learning by presenting examples in order of difficulty.
    
    Attributes:
        difficulty_scorer (Callable): Function to score examples by difficulty
        initial_easy_ratio (float): Initial ratio of easy examples to use
        final_easy_ratio (float): Final ratio of easy examples to use
        total_steps (int): Total number of steps to transition from initial to final ratio
        current_step (int): Current step in the curriculum
    """
    
    def __init__(
        self,
        difficulty_scorer: Callable = None,
        initial_easy_ratio: float = 0.8,
        final_easy_ratio: float = 0.2,
        total_steps: int = 100
    ):
        """
        Initialize the curriculum learning scheduler.
        
        Args:
            difficulty_scorer: Function to score examples by difficulty
            initial_easy_ratio: Initial ratio of easy examples to use
            final_easy_ratio: Final ratio of easy examples to use
            total_steps: Total number of steps to transition from initial to final ratio
        """
        self.difficulty_scorer = difficulty_scorer
        self.initial_easy_ratio = initial_easy_ratio
        self.final_easy_ratio = final_easy_ratio
        self.total_steps = total_steps
        self.current_step = 0
        self.difficulty_scores = None
        
        logger.info(f"Initialized CurriculumLearning with initial_easy_ratio={initial_easy_ratio}, "
                  f"final_easy_ratio={final_easy_ratio}, total_steps={total_steps}")
    
    def score_dataset(self, dataset: Dataset, model: nn.Module = None) -> np.ndarray:
        """
        Score examples in the dataset by difficulty.
        
        Args:
            dataset: Dataset to score
            model: Model used to evaluate difficulty (optional)
            
        Returns:
            Array of difficulty scores
        """
        if self.difficulty_scorer is None:
            logger.warning("No difficulty scorer provided. Using random scores.")
            return np.random.random(len(dataset))
        
        if self.difficulty_scores is not None:
            logger.info("Using cached difficulty scores")
            return self.difficulty_scores
        
        logger.info("Scoring dataset examples by difficulty")
        self.difficulty_scores = self.difficulty_scorer(dataset, model)
        return self.difficulty_scores
    
    def get_curriculum_sampler(
        self,
        dataset: Dataset,
        model: nn.Module = None,
        rescore: bool = False
    ) -> WeightedRandomSampler:
        """
        Get a weighted sampler based on current curriculum stage.
        
        Args:
            dataset: Dataset to sample from
            model: Model used to evaluate difficulty (optional)
            rescore: Whether to rescore the dataset
            
        Returns:
            Weighted random sampler for curriculum learning
        """
        # Score or rescore dataset if needed
        if self.difficulty_scores is None or rescore:
            self.difficulty_scores = self.score_dataset(dataset, model)
        
        # Calculate current easy ratio
        if self.current_step >= self.total_steps:
            easy_ratio = self.final_easy_ratio
        else:
            # Linear interpolation between initial and final ratios
            progress = self.current_step / self.total_steps
            easy_ratio = self.initial_easy_ratio - progress * (self.initial_easy_ratio - self.final_easy_ratio)
        
        # Sort indices by difficulty scores
        sorted_indices = np.argsort(self.difficulty_scores)
        num_examples = len(sorte

<div style="text-align: center">⁂</div>

[^1]: paste.txt
[^2]: Hybrid-Synergy-System_Phoenix-DemiGod-Cyberglitchcore-Live-Set.pdf
[^3]: Estructura-de-Archivos-y-Codigo-de-Phoenix-DemiGod-copia.pdf
[^4]: Phoenix-DemiGod-Cyberglitchcore-SetLive_-Integra.pdf
[^5]: COMPLETO.pdf
[^6]: COMPLETOV2.pdf
[^7]: Phoenix-DemiGod-Complete-Research-Paper_Redacted.pdf
[^8]: Folleto.docx
[^9]: paste-4.txt```

