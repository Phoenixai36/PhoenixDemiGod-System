# Phoenix DemiGod Model Router

Router inteligente multi-modelo para Phoenix Hydra con arquitectura Mamba/SSM, procesamiento 100% local y fallback automÃ¡tico.

## ğŸ¯ **CaracterÃ­sticas Principales**

- **Modelos Mamba/SSM**: 60-70% menos consumo energÃ©tico vs transformers
- **Procesamiento 100% Local**: Cero dependencias cloud via Ollama
- **Fallback AutomÃ¡tico**: Garantiza respuesta incluso si modelo principal falla
- **MonitorizaciÃ³n Completa**: MÃ©tricas Prometheus/Grafana para grants y auditorÃ­as
- **Router Inteligente**: SelecciÃ³n automÃ¡tica de modelo segÃºn tipo de tarea

## ğŸš€ **Setup RÃ¡pido**

### 1. Instalar Ollama

```bash
# Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Descargar desde https://ollama.ai
```

### 2. Ejecutar Setup AutomÃ¡tico

```bash
# Hacer ejecutable (Linux/macOS)
chmod +x src/phoenix_system_review/mamba_integration/setup_phoenix_models.sh

# Ejecutar setup
./src/phoenix_system_review/mamba_integration/setup_phoenix_models.sh
```

### 3. Instalar Dependencias Python

```bash
pip install -r src/phoenix_system_review/mamba_integration/requirements.txt
```

### 4. Iniciar Router

```bash
python src/phoenix_system_review/mamba_integration/phoenix_model_router.py
```

## ğŸ§ª **Test Inmediato**

```bash
# Test bÃ¡sico
curl -X POST http://localhost:8000/phoenixquery \
     -H "Content-Type: application/json" \
     -d '{"task": "Explica la eficiencia de Mamba vs Transformers"}'

# Test con script automÃ¡tico
python test_phoenix_router.py
```

## ğŸ“Š **MonitorizaciÃ³n**

### Endpoints Disponibles

- **Health Check**: `GET /health`
- **Performance Stats**: `GET /stats`
- **Available Models**: `GET /models`
- **Prometheus Metrics**: `GET /metrics`
- **API Documentation**: `GET /docs`

### Iniciar Grafana/Prometheus

```bash
docker-compose -f docker-compose.monitoring.yml up -d

# Acceder a:
# Grafana: http://localhost:3000 (admin/phoenix123)
# Prometheus: http://localhost:9090
```

## ğŸ¤– **Modelos Configurados**

| Modelo                | Tipo            | Uso Principal        | Eficiencia EnergÃ©tica |
| --------------------- | --------------- | -------------------- | --------------------- |
| `deepseek-coder:6.7b` | Code Specialist | AnÃ¡lisis de cÃ³digo   | âš¡ Alta                |
| `llama3.2:8b`         | Reasoning       | Razonamiento general | ğŸ”‹ Media               |
| `llama3.2:3b`         | Fallback        | Respaldo ligero      | âš¡ Alta                |
| `qwen2.5-coder:7b`    | Code Specialist | CÃ³digo avanzado      | ğŸ”‹ Media               |

## ğŸ›ï¸ **ConfiguraciÃ³n**

### Variables de Entorno (.env)

```bash
# Modelos
DEFAULTMODEL=deepseek-coder:6.7b
AGENTICMODEL=llama3.2:8b
FALLBACKMODEL=llama3.2:3b
SPECIALISTMODEL=qwen2.5-coder:7b

# ConfiguraciÃ³n
QUANTIZATION=4bit
INFERENCEMODE=LOCAL
AGENTMODE=true
ENABLEFALLBACK=true

# Ollama
OLLAMA_BASE_URL=http://localhost:11434

# MonitorizaciÃ³n
PROMETHEUS_ENABLED=true
ENERGY_REDUCTION_TARGET=65
MAX_WATTS_PER_INFERENCE=150
```

## ğŸ“ˆ **MÃ©tricas de Rendimiento**

### Objetivos de Eficiencia

- **ReducciÃ³n EnergÃ©tica**: 60-70% vs transformers tradicionales
- **Latencia**: <2 segundos para consultas tÃ­picas
- **Disponibilidad**: >99% con fallback automÃ¡tico
- **Procesamiento Local**: 100% sin dependencias cloud

### MÃ©tricas Prometheus

```
# Inferencias totales por modelo y tipo de tarea
phoenix_inferences_total{model="deepseek-coder", task_type="code_analysis"}

# DuraciÃ³n de inferencia
phoenix_inference_duration_seconds{model="deepseek-coder"}

# Consumo energÃ©tico actual
phoenix_energy_consumption_wh

# Fallbacks ejecutados
phoenix_fallbacks_total{from_model="deepseek-coder", to_model="llama3.2"}
```

## ğŸ”§ **API Usage**

### Consulta BÃ¡sica

```python
import httpx

async def query_phoenix(task: str):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/phoenixquery",
            json={
                "task": task,
                "task_type": "general_query",
                "enable_fallback": True
            }
        )
        return response.json()

# Ejemplo
result = await query_phoenix("Analiza este cÃ³digo Python")
print(f"Respuesta: {result['response']}")
print(f"Modelo usado: {result['model_used']}")
print(f"EnergÃ­a consumida: {result['energy_consumed_wh']}Wh")
```

### Tipos de Tareas Soportadas

- `code_analysis`: AnÃ¡lisis de cÃ³digo
- `system_review`: RevisiÃ³n de sistemas
- `reasoning`: Razonamiento y lÃ³gica
- `general_query`: Consultas generales
- `configuration`: ConfiguraciÃ³n de sistemas
- `security_audit`: AuditorÃ­as de seguridad

## ğŸ—ï¸ **Arquitectura**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚    â”‚  Model Router    â”‚    â”‚     Ollama      â”‚
â”‚   /phoenixquery â”‚â”€â”€â”€â–¶â”‚  - Task Analysis â”‚â”€â”€â”€â–¶â”‚  - deepseek     â”‚
â”‚   /health       â”‚    â”‚  - Model Select  â”‚    â”‚  - llama3.2     â”‚
â”‚   /stats        â”‚    â”‚  - Fallback      â”‚    â”‚  - qwen2.5      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Prometheus    â”‚    â”‚  Energy Monitor  â”‚    â”‚  Local Storage  â”‚
â”‚   Metrics       â”‚    â”‚  Performance     â”‚    â”‚  Models & Cache â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ **Casos de Uso**

### 1. AnÃ¡lisis de CÃ³digo Phoenix Hydra

```bash
curl -X POST http://localhost:8000/phoenixquery \
     -H "Content-Type: application/json" \
     -d '{
       "task": "Analiza este componente de Phoenix Hydra y sugiere mejoras",
       "task_type": "code_analysis"
     }'
```

### 2. RevisiÃ³n de Configuraciones

```bash
curl -X POST http://localhost:8000/phoenixquery \
     -H "Content-Type: application/json" \
     -d '{
       "task": "Revisa esta configuraciÃ³n Docker Compose",
       "task_type": "configuration"
     }'
```

### 3. AuditorÃ­a de Seguridad

```bash
curl -X POST http://localhost:8000/phoenixquery \
     -H "Content-Type: application/json" \
     -d '{
       "task": "Identifica vulnerabilidades en este endpoint API",
       "task_type": "security_audit"
     }'
```

## ğŸ“‹ **Troubleshooting**

### Problemas Comunes

1. **Ollama no responde**
   ```bash
   # Verificar que Ollama estÃ¡ corriendo
   curl http://localhost:11434/api/tags
   
   # Reiniciar si es necesario
   ollama serve
   ```

2. **Modelo no encontrado**
   ```bash
   # Listar modelos disponibles
   ollama list
   
   # Descargar modelo faltante
   ollama pull deepseek-coder:6.7b
   ```

3. **Alto consumo de memoria**
   ```bash
   # Usar modelos mÃ¡s pequeÃ±os
   export DEFAULTMODEL=llama3.2:3b
   
   # Habilitar cuantizaciÃ³n
   export QUANTIZATION=4bit
   ```

### Logs y Debugging

```bash
# Ver logs del router
python phoenix_model_router.py --log-level debug

# Verificar health check
curl http://localhost:8000/health

# Ver estadÃ­sticas detalladas
curl http://localhost:8000/stats | jq
```

## ğŸ“ **Para Grants y AuditorÃ­as**

### DocumentaciÃ³n AutomÃ¡tica

El router genera automÃ¡ticamente:

- **MÃ©tricas de eficiencia energÃ©tica** vs transformers tradicionales
- **Logs de rendimiento** con timestamps y modelos utilizados
- **EstadÃ­sticas de disponibilidad** y fallback rates
- **Trazabilidad completa** de todas las inferencias

### Reportes para NEOTEC/ENISA

```python
# Generar reporte de eficiencia
stats = requests.get("http://localhost:8000/stats").json()

print(f"Eficiencia energÃ©tica: {stats['energy_efficiency_vs_transformer']}")
print(f"Disponibilidad del sistema: {stats['success_rate']:.2%}")
print(f"Procesamiento 100% local: âœ…")
```

## ğŸ”® **Roadmap**

- [ ] **IntegraciÃ³n RUBIK**: Agentes biomimÃ©tricos con ciclos de vida
- [ ] **Modelos Mamba nativos**: Cuando estÃ©n disponibles en Ollama
- [ ] **Auto-scaling**: Ajuste automÃ¡tico de recursos segÃºn carga
- [ ] **Multi-GPU**: DistribuciÃ³n de carga entre mÃºltiples GPUs
- [ ] **Edge deployment**: OptimizaciÃ³n para hardware edge/IoT

---

## ğŸ“ **Soporte**

Para issues y mejoras, crear ticket en el repositorio Phoenix Hydra con:

- Logs del router (`/health`, `/stats`)
- ConfiguraciÃ³n de modelos (`/models`)
- DescripciÃ³n del problema y pasos para reproducir

**Â¡Phoenix DemiGod estÃ¡ listo para procesamiento 100% local con eficiencia Mamba/SSM!** ğŸš€