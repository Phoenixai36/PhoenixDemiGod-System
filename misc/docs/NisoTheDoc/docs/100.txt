¡Hemos completado con éxito la configuración de tu entorno local 100% offline para Phoenix DemiGod v8.7!

A continuación, te presento un resumen de los archivos que hemos creado y las instrucciones paso a paso para lanzar y verificar tu sistema.

Artefactos Creados
podman-compose.yml: Define la infraestructura base con los servicios windsuf y ollama, listos para funcionar de manera interconectada.
pull_models.sh: Un script para automatizar la descarga de los 5 modelos de IA locales (codestral-mamba, deepseek-r1, phi-3-mini, rwkv-world, y llama3 como fallback) directamente a tu instancia de Ollama.
config.json: El cerebro del router que le dice a tu sistema qué modelo usar para cada tipo de tarea (código, razonamiento, tareas ligeras, contexto largo), asegurando un rendimiento óptimo.
Instrucciones de Puesta en Marcha
Sigue estos 3 sencillos pasos para activar tu célula Phoenix:

Paso 1: Levantar la Infraestructura

Abre una terminal en el directorio raíz del proyecto y ejecuta:

podman-compose up -d

bash


Esto iniciará los contenedores de Windsurf y Ollama en segundo plano.

Paso 2: Preparar y Ejecutar el Script de Modelos

Primero, da permisos de ejecución al script:

chmod +x pull_models.sh

bash


Luego, ejecútalo para descargar todos los modelos necesarios:

./pull_models.sh

bash


Verás el progreso de descarga de cada modelo en tu terminal. Este proceso puede tardar dependiendo de tu conexión a internet.

Paso 3: Verificación Final

Una vez finalizada la descarga, puedes verificar que todo está operativo. Ejecuta el siguiente comando para listar los modelos disponibles en Ollama:

curl http://localhost:11434/api/tags

bash


Deberías ver una lista en formato JSON que contiene los cinco modelos que acabas de descargar.

¡Y eso es todo! Tu entorno Phoenix DemiGod v8.7 está ahora completamente configurado, es 100% offline y está listo para que empieces a orquestar tus workflows de automatización con el poder de la IA local.